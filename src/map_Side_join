package BigDataHW2_1;

import java.io.*;
import java.util.*;

import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
//import org.apache.hadoop.mapreduce.Reducer.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class BigDataHW2_1 {
	private static int count=0;

    public static class MapAverage extends
            Mapper<LongWritable, Text, Text, DoubleWritable> {
        // private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();
        private DoubleWritable rating = new DoubleWritable();

        public void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {
            String line = value.toString();
            StringTokenizer tokenizer = new StringTokenizer(line, "::");
            if (tokenizer.countTokens() == 4) {
                tokenizer.nextToken();
                word.set(tokenizer.nextToken());
                rating.set(Double.parseDouble(tokenizer.nextToken()));
                context.write(word, rating);
                tokenizer.nextToken();
            }
        }
    }

    public static class ReduceAverage extends
            Reducer<Text, DoubleWritable, Text, NullWritable> {
        public void reduce(Text key, Iterable<DoubleWritable> values,
                Context context) throws IOException, InterruptedException {
            DoubleWritable average = new DoubleWritable();
            double sum = 0;
            double count = 0;
            for (DoubleWritable val : values) {
                sum += val.get();
                count++;
            }
            average.set(sum / count);
            context.write(new Text(key + "::" + average), NullWritable.get());
        }
    }

    public static class KeyComparator extends WritableComparator {
        protected KeyComparator() {
            super(DoubleWritable.class, true);
        }

        @SuppressWarnings("rawtypes")
        @Override
        public int compare(WritableComparable w1, WritableComparable w2) {
            DoubleWritable k1 = (DoubleWritable) w1;
            DoubleWritable k2 = (DoubleWritable) w2;

            return -1 * k1.compareTo(k2);
        }
    }

    public static class MapJoin extends
            Mapper<LongWritable, Text, DoubleWritable, Text> {

        private static Map<String, String> map = new HashMap<String, String>();;
        File f;
        @Override
        public void setup(Context context) {
            try
            {
                Configuration conf = context.getConfiguration();
                Path p[] = DistributedCache.getLocalCacheFiles(conf);
                f = new File(p[0].toString());
                BufferedReader br = new BufferedReader(new FileReader(f));
                String line="";
                while((line=br.readLine())!= null)
                {
                    String[] token = line.split("::", 2);
                    map.put(token[0], token[1]);
                }
                System.out.println(map);
            }
            catch(Exception e)
            {
                e.printStackTrace();
            }
        }
        
        public void map(LongWritable key, Text values, Context context)
                throws IOException, InterruptedException {
            Text MovieInfo=new Text();
             DoubleWritable rating = new DoubleWritable();
             String line = values.toString();
             String[] token = line.split("::",2);
            
             boolean write=false;

             boolean tf=context.getConfiguration().getBoolean("isRatingInMemory", true);
             if(tf)
             {
                 MovieInfo.set(new Text(token[0]+"\t"+token[1].replaceAll("::", "\t")));
                 try{rating.set(Double.parseDouble(map.get(token[0])));} catch(NullPointerException npe){rating.set(1.1);}
                 if(token[1].contains("Action")) write=true;
             }
             else
             {
             try{MovieInfo.set(new Text(token[0]+"\t"+map.get(token[0]).replaceAll("::", "\t")));}catch(NullPointerException npe){MovieInfo.set("No ivie info available");}
             try{rating.set(Double.parseDouble(token[1]));}catch(NullPointerException npe){rating.set(0.0);}
             try{
             if(map.get(token[0]).contains("Action")) write=true;
                 }catch(NullPointerException npe){}
             }
             if(write)
             {         context.write(rating,MovieInfo);

                    }
             
              
            
        }
    }

    public static class ReduceJoin extends
            Reducer<DoubleWritable, Text, Text, DoubleWritable> {
        DoubleWritable averageRating = new DoubleWritable();

        public void reduce(DoubleWritable key, Iterable<Text> values,
                Context context) throws IOException, InterruptedException {
            for (Text t : values) {
            	if(count<10)
            	{
            		context.write(t, key);
            		count++;
            	}
                
            }
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = new Job(conf, "BigDataHW2_1_1");
        Path moviesFile = new Path(args[0] + "movies");
        Path ratingsFile = new Path(args[0] + "ratings");
        Path tempOutput = new Path(args[1]);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(DoubleWritable.class);
        job.setJarByClass(BigDataHW2_1.class);

        job.setMapperClass(MapAverage.class);
        job.setReducerClass(ReduceAverage.class);

        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);

        FileInputFormat.addInputPath(job, ratingsFile);
        FileOutputFormat.setOutputPath(job, tempOutput);

        if (job.waitForCompletion(true)) 
        {
            Configuration conf2 = new Configuration();
            FileSystem fs = FileSystem.get(conf2);
            Path inputPath;
            Path Intermediate = new Path(args[1]+"/part-r-00000");
            Long len_intermediate = fs.getFileStatus(Intermediate).getLen();
            Long len_movies = fs.getFileStatus(moviesFile).getLen();
            if (len_intermediate <= len_movies) {
                DistributedCache.addCacheFile(Intermediate.toUri(), conf2);
                inputPath = moviesFile;
                conf2.setBoolean("isRatingInMemory", true);
            } 
            else 
            {
                DistributedCache.addCacheFile(moviesFile.toUri(), conf2);
                inputPath = Intermediate;
                conf2.setBoolean("isRatingInMemory", false);
            }
            DistributedCache.addCacheFile(Intermediate.toUri(), conf2);
            Job job2 = new Job(conf2, "BigDataHW2_1_2");
            
            job2.setJarByClass(BigDataHW2_1.class);
            job2.setMapperClass(MapJoin.class);
            job2.setReducerClass(ReduceJoin.class);
            job2.setMapOutputKeyClass(DoubleWritable.class);
            job2.setMapOutputValueClass(Text.class);
            job2.setInputFormatClass(TextInputFormat.class);
            job2.setOutputFormatClass(TextOutputFormat.class);
            job2.setOutputKeyClass(Text.class);
            job2.setOutputValueClass(DoubleWritable.class);
            job2.setSortComparatorClass(KeyComparator.class);

            FileInputFormat.addInputPath(job2, inputPath);
            FileOutputFormat.setOutputPath(job2, new Path(args[2]));

            job2.waitForCompletion(true);

        }
    }

}
